{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is for combining the entries from linkedin jobs files - dataAnalyst_scraped.csv, dataScientist_scraped.csv, machineLearningEngineer_scraped.csv, softwareDeveloper_scraped.csv\n",
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'softwareDeveloper_scraped.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34544\\2242520945.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataScientist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataScientist_scraped.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmachineLearningEngineer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'machineLearningEngineer_scraped.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msoftwareDeveloper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softwareDeveloper_scraped.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Jason\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jason\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jason\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jason\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jason\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'softwareDeveloper_scraped.csv'"
     ]
    }
   ],
   "source": [
    "# import files\n",
    "dataAnalyst = pd.read_csv('dataAnalyst_scraped.csv')\n",
    "dataScientist = pd.read_csv('dataScientist_scraped.csv')\n",
    "machineLearningEngineer = pd.read_csv('machineLearningEngineer_scraped.csv')\n",
    "softwareDeveloper = pd.read_csv('softwareDeveloper_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351 entries, 0 to 350\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       351 non-null    int64  \n",
      " 1   Job_ID           351 non-null    int64  \n",
      " 2   Job_txt          320 non-null    object \n",
      " 3   company          249 non-null    object \n",
      " 4   job-title        320 non-null    object \n",
      " 5   level            320 non-null    object \n",
      " 6   location         320 non-null    object \n",
      " 7   posted-time-ago  317 non-null    object \n",
      " 8   nb_candidats     142 non-null    float64\n",
      " 9   scraping_date    351 non-null    object \n",
      " 10  posted_date      317 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 30.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# check files \n",
    "dataAnalyst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 375 entries, 0 to 374\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       375 non-null    int64  \n",
      " 1   Job_ID           375 non-null    int64  \n",
      " 2   Job_txt          258 non-null    object \n",
      " 3   company          231 non-null    object \n",
      " 4   job-title        258 non-null    object \n",
      " 5   level            258 non-null    object \n",
      " 6   location         258 non-null    object \n",
      " 7   posted-time-ago  249 non-null    object \n",
      " 8   nb_candidats     133 non-null    float64\n",
      " 9   scraping_date    375 non-null    object \n",
      " 10  posted_date      249 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 32.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataScientist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       400 non-null    int64  \n",
      " 1   Job_ID           400 non-null    int64  \n",
      " 2   Job_txt          318 non-null    object \n",
      " 3   company          296 non-null    object \n",
      " 4   job-title        318 non-null    object \n",
      " 5   level            318 non-null    object \n",
      " 6   location         318 non-null    object \n",
      " 7   posted-time-ago  306 non-null    object \n",
      " 8   nb_candidats     172 non-null    float64\n",
      " 9   scraping_date    400 non-null    object \n",
      " 10  posted_date      306 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 34.5+ KB\n"
     ]
    }
   ],
   "source": [
    "machineLearningEngineer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25 entries, 0 to 24\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       25 non-null     int64  \n",
      " 1   Job_ID           25 non-null     int64  \n",
      " 2   Job_txt          20 non-null     object \n",
      " 3   company          20 non-null     object \n",
      " 4   job-title        20 non-null     object \n",
      " 5   level            20 non-null     object \n",
      " 6   location         20 non-null     object \n",
      " 7   posted-time-ago  20 non-null     object \n",
      " 8   nb_candidats     8 non-null      float64\n",
      " 9   scraping_date    25 non-null     object \n",
      " 10  posted_date      20 non-null     object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "softwareDeveloper.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns - posted-time-ago, nb_candidats, scraping_date, posted_date\n",
    "dataAnalyst = dataAnalyst.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "dataScientist = dataScientist.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "machineLearningEngineer = machineLearningEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "softwareDeveloper = softwareDeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "\n",
    "# drop na values \n",
    "dataAnalyst = dataAnalyst.dropna()\n",
    "dataScientist = dataScientist.dropna()\n",
    "machineLearningEngineer = machineLearningEngineer.dropna()\n",
    "softwareDeveloper = softwareDeveloper.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform concatenation\n",
    "linkedin_jobs = pd.DataFrame(columns=dataAnalyst.columns)\n",
    "\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, dataAnalyst], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, dataScientist], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, machineLearningEngineer], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, softwareDeveloper], ignore_index=True)\n",
    "\n",
    "# drop duplicated columns \n",
    "linkedin_jobs = linkedin_jobs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Compliance and Access Operations Senior A...</td>\n",
       "      <td>ByteDance</td>\n",
       "      <td>Data Compliance and Access Operations Senior A...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intelligence Analyst, Rights Protection (South...</td>\n",
       "      <td>Sportradar</td>\n",
       "      <td>Intelligence Analyst, Rights Protection (South...</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Analyst, Data Science, Health Services ...</td>\n",
       "      <td>SingHealth</td>\n",
       "      <td>Senior Analyst, Data Science, Health Services ...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - Marketing Analytics, Regional B...</td>\n",
       "      <td>Shopee</td>\n",
       "      <td>Data Analyst - Marketing Analytics, Regional B...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead BI Analyst Dyson Singapore, Singapore 1 m...</td>\n",
       "      <td>Dyson</td>\n",
       "      <td>Lead BI Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_txt     company  \\\n",
       "0  Data Compliance and Access Operations Senior A...   ByteDance   \n",
       "1  Intelligence Analyst, Rights Protection (South...  Sportradar   \n",
       "2  Senior Analyst, Data Science, Health Services ...  SingHealth   \n",
       "3  Data Analyst - Marketing Analytics, Regional B...      Shopee   \n",
       "4  Lead BI Analyst Dyson Singapore, Singapore 1 m...       Dyson   \n",
       "\n",
       "                                           job-title             level  \\\n",
       "0  Data Compliance and Access Operations Senior A...  Mid-Senior level   \n",
       "1  Intelligence Analyst, Rights Protection (South...         Associate   \n",
       "2  Senior Analyst, Data Science, Health Services ...  Mid-Senior level   \n",
       "3  Data Analyst - Marketing Analytics, Regional B...       Entry level   \n",
       "4                                    Lead BI Analyst  Mid-Senior level   \n",
       "\n",
       "               location  \n",
       "0  Singapore, Singapore  \n",
       "1  Singapore, Singapore  \n",
       "2  Singapore, Singapore  \n",
       "3  Singapore, Singapore  \n",
       "4  Singapore, Singapore  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_jobs.to_csv('linkedin_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for reproducibility \n",
    "def combine_linkedin_jobs(dataAnalyst_path='dataanalyst_scraped.csv', dataScientist_path='datascientist_scraped.csv', machineLearningEngineer_path='machinelearningengineer_scraped.csv', softwareengineer_path='softwareengineer_scraped.csv', AIdeveloper_path='AIdeveloper_scraped.csv', dataEngineer_path='dataengineer_scraped.csv'):\n",
    "    dataAnalyst = pd.read_csv(dataAnalyst_path, encoding='utf-8')\n",
    "    dataScientist = pd.read_csv(dataScientist_path, encoding='utf-8')\n",
    "    machineLearningEngineer = pd.read_csv(machineLearningEngineer_path, encoding='utf-8')\n",
    "    softwareDeveloper = pd.read_csv(softwareengineer_path, encoding='utf-8')\n",
    "    AIdeveloper = pd.read_csv(AIdeveloper_path, encoding='utf-8')\n",
    "    dataEngineer = pd.read_csv(dataEngineer_path, encoding='utf-8')\n",
    "\n",
    "    # drop unused columns - posted-time-ago, nb_candidats, scraping_date, posted_date\n",
    "    dataAnalyst = dataAnalyst.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    dataScientist = dataScientist.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    machineLearningEngineer = machineLearningEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    softwareDeveloper = softwareDeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    AIdeveloper = AIdeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    dataEngineer = dataEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    \n",
    "    # drop na values \n",
    "    dataAnalyst = dataAnalyst.dropna()\n",
    "    dataScientist = dataScientist.dropna()\n",
    "    machineLearningEngineer = machineLearningEngineer.dropna()\n",
    "    softwareDeveloper = softwareDeveloper.dropna()\n",
    "    AIdeveloper = AIdeveloper.dropna()\n",
    "    dataEngineer = dataEngineer.dropna()\n",
    "\n",
    "    # perform splitting to keep only valuable information \n",
    "    dataAnalyst['Job_txt'] = dataAnalyst['Job_txt'].apply(lambda x : x.split('Report this job')[1].replace(\"â€™\", \"'\"))\n",
    "    dataAnalyst['Job_txt'] = dataAnalyst['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "    dataScientist['Job_txt'] = dataScientist['Job_txt'].apply(lambda x : x.split('Report this job')[1])\n",
    "    dataScientist['Job_txt'] = dataScientist['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "    machineLearningEngineer['Job_txt'] = machineLearningEngineer['Job_txt'].apply(lambda x : x.split('Report this job')[1])\n",
    "    machineLearningEngineer['Job_txt'] = machineLearningEngineer['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "    softwareDeveloper['Job_txt'] = softwareDeveloper['Job_txt'].apply(lambda x : x.split('Report this job')[1])\n",
    "    softwareDeveloper['Job_txt'] = softwareDeveloper['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "    AIdeveloper['Job_txt'] = AIdeveloper['Job_txt'].apply(lambda x : x.split('Report this job')[1])\n",
    "    AIdeveloper['Job_txt'] = AIdeveloper['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "    dataEngineer['Job_txt'] = dataEngineer['Job_txt'].apply(lambda x : x.split('Report this job')[1])\n",
    "    dataEngineer['Job_txt'] = dataEngineer['Job_txt'].apply(lambda x : x.split('Show more Show less')[0])\n",
    "\n",
    "    # perform concatenation\n",
    "    linkedin_jobs = pd.DataFrame(columns=dataAnalyst.columns)\n",
    "    \n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataAnalyst], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataScientist], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, machineLearningEngineer], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, softwareDeveloper], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, AIdeveloper], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataEngineer], ignore_index=True)\n",
    "\n",
    "    # drop duplicated columns \n",
    "    linkedin_jobs = linkedin_jobs.drop_duplicates()\n",
    "\n",
    "    # export to csv\n",
    "    linkedin_jobs.to_json(\"linkedin_jobs.json\")\n",
    "    linkedin_jobs.to_csv('linkedin_jobs.csv')\n",
    "    return linkedin_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1087 entries, 0 to 1574\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   url        1087 non-null   object\n",
      " 1   Job_txt    1087 non-null   object\n",
      " 2   company    1087 non-null   object\n",
      " 3   job-title  1087 non-null   object\n",
      " 4   level      1087 non-null   object\n",
      " 5   location   1087 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 91.7+ KB\n"
     ]
    }
   ],
   "source": [
    "combine_linkedin_jobs().info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
